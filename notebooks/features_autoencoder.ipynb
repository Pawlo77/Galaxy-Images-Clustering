{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "<FloatDTypePolicy \"float32\">\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Reshape\n",
    "from keras.losses import Huber\n",
    "from keras.activations import swish\n",
    "from keras.initializers import HeNormal\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "from keras import mixed_precision\n",
    "from tqdm import tqdm\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(f\"Using device: {physical_devices[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU device found, using CPU\")\n",
    "\n",
    "print(mixed_precision.global_policy())\n",
    "# mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "from models import create_dataset_from_mapping, load_entire, encode\n",
    "\n",
    "root_dir = os.path.abspath(os.path.join(\".\", \"..\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up further training we'll load entire dataset into memory. `create_dataset_from_mapping` on load reshapes features back to 4, 4, 1536 and fits 2 average poolings on top of it to reduce size to 4 * 1536. Final dataset is 2D array of shape (n_samples, 4 * 1536) - so we have up to now 1 - (1536 * 4) / (150 * 150 * 3) = ~90% reduction of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, train_size = create_dataset_from_mapping(mapping_file=\"train_mapping.csv\")\n",
    "# validation_dataset, validation_size = create_dataset_from_mapping(mapping_file=\"validation_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, y_train = load_entire(train_dataset, validation_size)\n",
    "# np.save(os.path.join(root_dir, \"data\", \"X_train.npy\"), X_train)\n",
    "# np.save(os.path.join(root_dir, \"data\", \"y_train.npy\"), y_train)\n",
    "# del y_train\n",
    "# del train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_valid, y_valid = load_entire(validation_dataset, validation_size)\n",
    "# np.save(os.path.join(root_dir, \"data\", \"X_valid.npy\"), X_valid)\n",
    "# np.save(os.path.join(root_dir, \"data\", \"y_valid.npy\"), y_valid)\n",
    "# del y_valid\n",
    "# del validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding features with autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 17:49:42.350163: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-06-03 17:49:42.350179: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-03 17:49:42.350184: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-03 17:49:42.350200: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-03 17:49:42.350211: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Define the encoder\n",
    "encoder = Sequential([\n",
    "    Dense(6144, input_shape=(6144, ), kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(2048, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1024, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    # Dense(512, kernel_initializer=HeNormal()),\n",
    "    # Activation(swish),\n",
    "    # BatchNormalization(),\n",
    "    # Dropout(0.3),\n",
    "    # Dense(512, kernel_initializer=HeNormal()),\n",
    "    # Activation(swish),\n",
    "    # BatchNormalization(),\n",
    "    # Dropout(0.3),\n",
    "    Dense(512, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3)\n",
    "])\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(256, input_shape=(256,), kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    # Dense(512, kernel_initializer=HeNormal()),\n",
    "    # Activation(swish),\n",
    "    # BatchNormalization(),\n",
    "    # Dropout(0.3),\n",
    "    # Dense(512, kernel_initializer=HeNormal()),\n",
    "    # Activation(swish),\n",
    "    # BatchNormalization(),\n",
    "    # Dropout(0.3),\n",
    "    Dense(1024, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(2048, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(6144, kernel_initializer=HeNormal()),\n",
    "    Activation(swish),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "])\n",
    "\n",
    "autoencoder = Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ExponentialDecay(\n",
    "    initial_learning_rate=0.003,\n",
    "    decay_steps=1500,\n",
    "    decay_rate=0.9,\n",
    ")\n",
    "optimizer = Nadam(learning_rate=lr_scheduler)\n",
    "autoencoder.compile(optimizer=optimizer, loss=Huber())\n",
    "# autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "checkpoint_dir = os.path.join(\".\", \"logs\", \"checkpoints\")\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"autoencoder_{epoch:02d}.weights.h5\")\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, save_weights_only=True, save_freq=\"epoch\"\n",
    ")\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1, restore_best_weights=True)\n",
    "tensorboard = TensorBoard(log_dir=\"./logs\", histogram_freq=1)\n",
    "callbacks_list = [early_stopping, tensorboard, checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our limited performance we commented out part of the network to speed up training and below code uses only quater of the training set (mode=2) ie around 50k images. It also uses relatively large batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from checkpoint ./logs/checkpoints/autoencoder_08.weights.h5\n"
     ]
    }
   ],
   "source": [
    "initial_epoch = 0\n",
    "checkpoints = sorted(os.listdir(checkpoint_dir))\n",
    "if len(checkpoints):\n",
    "    latest_checkpoint = os.path.join(checkpoint_dir, checkpoints[-1])\n",
    "    autoencoder.load_weights(latest_checkpoint)\n",
    "    initial_epoch = int(os.path.basename(latest_checkpoint).split(\"_\")[-1].split(\".\")[0])\n",
    "    print(f\"Loaded from checkpoint {latest_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 17:49:44.237624: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 303ms/step - loss: 0.0091 - val_loss: 2.9995\n",
      "Epoch 10/500\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 295ms/step - loss: 0.0091 - val_loss: 210262.1406\n",
      "Epoch 11/500\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 299ms/step - loss: 0.0091 - val_loss: 24.0805\n",
      "Epoch 12/500\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 298ms/step - loss: 0.0091 - val_loss: 17.2985\n",
      "Epoch 13/500\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 300ms/step - loss: 0.0091 - val_loss: 18.0310\n",
      "Epoch 14/500\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 301ms/step - loss: 0.0091 - val_loss: 13.9465\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x349b083a0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "# autoencoder.fit(X_train, X_train, epochs=epochs, callbacks=callbacks_list, batch_size=16, validation_data=(X_valid, X_valid))\n",
    "\n",
    "train_dataset, train_size = create_dataset_from_mapping(mapping_file=\"train_mapping.csv\", mode=2, max_size=50000)\n",
    "validation_dataset, validation_size = create_dataset_from_mapping(mapping_file=\"validation_mapping.csv\", mode=2, max_size=10000)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "validation_dataset = validation_dataset.batch(256).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "autoencoder.fit(train_dataset, epochs=epochs, callbacks=callbacks_list, validation_data=validation_dataset, initial_epoch=initial_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.save(os.path.join(root_dir, \"models\", \"encoder.keras\"))\n",
    "decoder.save(os.path.join(root_dir, \"models\", \"decoder.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_dataset\n",
    "del validation_dataset\n",
    "del decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding validation:  99%|█████████▉| 85/86 [00:10<00:00,  7.88it/s]2024-06-03 18:22:45.976025: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Encoding validation: 100%|██████████| 86/86 [00:10<00:00,  8.47it/s]\n",
      "Encoding train: 100%|█████████▉| 770/771 [01:33<00:00,  7.34it/s]2024-06-03 18:24:20.045446: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "Encoding train: 100%|██████████| 771/771 [01:33<00:00,  8.22it/s]\n"
     ]
    }
   ],
   "source": [
    "encode(\"validation\", encoder)\n",
    "encode(\"train\", encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dataset is 2D array of shape (n_samples, 256) - so we have up to now 1 - (256) / (150 * 150 * 3) = ~99.6% reduction of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
