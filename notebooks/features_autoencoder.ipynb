{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(\"..\", \"src\")))\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, Activation, Reshape\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.optimizers import Nadam\n",
    "from keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(f\"Using device: {physical_devices[0]}\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU device found, using CPU\")\n",
    "\n",
    "from models import create_dataset_from_mapping, load_entire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed up further training we'll load entire dataset into memory. `create_dataset_from_mapping` on load reshapes features back to 4, 4, 1536 and fits 2 average poolings on top of it to reduce size to 4 * 1536. Final dataset is 2D array of shape (n_samples, 4 * 1536) - so we have up to now 1 - (1536 * 4) / (150 * 150 * 3) = ~90% reduction of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 11:01:33.002318: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1\n",
      "2024-06-03 11:01:33.002361: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-06-03 11:01:33.002367: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-06-03 11:01:33.002597: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-06-03 11:01:33.002612: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, train_size = create_dataset_from_mapping(mapping_file=\"train_mapping.csv\")\n",
    "validation_dataset, validation_size = create_dataset_from_mapping(mapping_file=\"validation_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 11:03:03.238319: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_entire(train_dataset, validation_size)\n",
    "X_valid, y_valid = load_entire(validation_dataset, validation_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([21909, 6144])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding features with autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GELU activation function\n",
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "\n",
    "# Define the encoder\n",
    "encoder = Sequential([\n",
    "    Dense(6144, input_shape=(6144, )),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(2048),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1024),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(256),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3)\n",
    "])\n",
    "\n",
    "decoder = Sequential([\n",
    "    Dense(256, input_shape=(256,)),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(512),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(1024),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(2048),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(6144),\n",
    "    Activation(gelu),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "])\n",
    "\n",
    "autoencoder = Sequential([encoder, decoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = ExponentialDecay(\n",
    "    initial_learning_rate=0.003,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9,\n",
    ")\n",
    "optimizer = Nadam(learning_rate=lr_scheduler)\n",
    "autoencoder.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "callbacks_list = [early_stopping, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-03 10:25:27.974471: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "\n",
    "autoencoder.fit(X_train, X_train, epochs=epochs, callbacks=callbacks_list, batch_size=16, validation_data=(X_valid, X_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.path.join(\".\", \"..\"))\n",
    "\n",
    "encoder.save(os.path.join(root_dir, \"models\", \"encoder.h5\"))\n",
    "decoder.save(os.path.join(root_dir, \"models\", \"decoder.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = encoder.predict(X_train)\n",
    "X_valid_encoded = encoder.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(root_dir, \"data\", \"X_train_encoded.npy\"), X_train_encoded)\n",
    "np.save(os.path.join(root_dir, \"data\", \"X_valid_encoded.npy\"), X_valid_encoded)\n",
    "np.save(os.path.join(root_dir, \"data\", \"y_train.npy\"), y_train)\n",
    "np.save(os.path.join(root_dir, \"data\", \"y_valid.npy\"), y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final dataset is 2D array of shape (n_samples, 256) - so we have up to now 1 - (256) / (150 * 150 * 3) = ~99.6% reduction of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
